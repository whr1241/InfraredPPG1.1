#!/usr/bin/env python
# -*- coding  : utf-8 -*-
# @Time       : 2022/3/9 22:19
# @Author     : wanghaoran
# @Site       : SCNU
# @File       : signal_tools.py
# @Description: åç»­ä¿¡å·å¤„ç†çš„å„ç§å‡½æ•°
# @Software   : PyCharm
import cv2
import matplotlib.pyplot as plt
import numpy as np
import scipy.linalg as sl  # åº”ç”¨äºçº¿æ€§ä»£æ•°ç¨‹åºé¢†åŸŸ
from scipy import signal
from sklearn.decomposition import PCA
from sklearn.decomposition import FastICA
from sklearn.decomposition import FastICA
# from filterpy.kalman import KalmanFilter
# from filterpy.common import Q_discrete_white_noise
import pywt


"""
1ã€å·´ç‰¹æ²ƒæ–¯æ»¤æ³¢
ç§»åŠ¨å¹³å‡æ»¤æ³¢
æ»‘åŠ¨çª—å£æ»¤æ³¢ç®—æ³•
å…¨å±€è‡ªç›¸å…³æ»¤æ³¢
SPAå¹³æ»‘å…ˆéªŒè¶‹åŠ¿å»é™¤
å¹…å€¼æ»¤æ³¢
åŒæ€æ»¤æ³¢
é€‰æ‹©æœ€å…·æœ‰å‘¨æœŸæ€§çš„ä¿¡å·
FFTå˜æ¢
é¢‘åŸŸæœ€å¤§å€¼å¯¹åº”çš„é¢‘ç‡
å‡æ–¹æ ¹è¯¯å·®(RMSE)è®¡ç®—
"""


# å½“å‰ä½¿ç”¨
def fftTransfer(data, N=1024):
    """
    FFTå˜æ¢
    :return:
    TODO: å¯èƒ½éœ€è¦æ”¹è¿›ä¸€ä¸‹ï¼Œä»é¢‘è°±å›¾åˆ°åŠŸç‡è°±å›¾ã€‚å¥½åƒåˆä¸€æ ·
    """
    if len(data) < N:
        for _ in range(N - len(data)):  # è¡¥é›¶è¡¥è‡³Nç‚¹
            data.append(0)
    else:
        data = data[0:N]
    df = [30 / N * i for i in range(N)]  # é¢‘è°±åˆ†è¾¨ç‡ N=1024,dfä¹Ÿåº”è¯¥æ˜¯1024é•¿. 0åˆ°30
    fft_data = np.abs(np.fft.fft(data))  # typeæ˜¯array, shapeæ˜¯1024é•¿

    fft_data = fft_data.tolist()  # çºµåæ ‡

    # å‰120ä¸ªé‡Œé¢çš„æå€¼ç‚¹æ¨ªåæ ‡
    num_peak = signal.find_peaks(fft_data[0:120], distance=2) #distanceè¡¨æå¤§å€¼ç‚¹çš„è·ç¦»è‡³å°‘å¤§äºç­‰äº2ä¸ªæ°´å¹³å•ä½
    num_peak_list_X = num_peak[0].tolist()

    # å¾—åˆ°æå€¼ç‚¹çš„çºµåæ ‡
    num_peak_list_Y = []
    for i in num_peak_list_X:
        num_peak_list_Y.append(fft_data[i])

    # å–æœ€å¤§çš„äº”ä¸ªçºµåæ ‡
    num_peak_list_Y_final = sorted(num_peak_list_Y, reverse=True)[0:10]

    final_hr = []
    num_peak_list_X_final = []
    for i in num_peak_list_Y_final:    
        final_hr.append(df[fft_data[0:1024].index(i)]*60)
        num_peak_list_X_final.append(df[fft_data[0:1024].index(i)])  # æå€¼ç‚¹æ¨ªåæ ‡

    hr = df[fft_data[0:1024].index(max(fft_data[0:120]))]*60

    # å½“ä¿¡å·å¾ˆæ¸…æ™°æ—¶ï¼Œä¸»å³°å¾ˆæ˜æ˜¾ï¼Œç›´æ¥åªå–ä¸»å³°å¥½äº†
    # TODOï¼šå‡å³°æœ‰æ—¶å€™ä¹Ÿä¼šå˜å¾—é«˜å‡ºçœŸå³°å¾ˆå¤šå€
    # TODOï¼šè¿˜æ˜¯åº”è¯¥æŒ‰ç…§åŠŸç‡æ¯”å€¼é˜ˆå€¼æ¥åˆ¤æ–­å§
    # if num_peak_list_Y_final[0] > num_peak_list_Y_final[1]*2:
    #     final_hr = [hr]

    # showä¸€ä¸‹
    plt.figure('FFT')
    plt.plot(df, fft_data)
    plt.axis([0, 5, 0, np.max(fft_data)*2])
    plt.title('Heart Rate estimate: {:.2f}'.format(hr))
    for ii in range(len(num_peak_list_Y_final)):  # ç”»å‡º5ä¸ªæå€¼ç‚¹
        plt.plot(num_peak_list_X_final[ii], num_peak_list_Y_final[ii],'*',markersize=10)
    plt.pause(0.1)	# pause 1 second
    plt.clf()		# clear the current figure

    return hr, final_hr



def SPA(data):
    """
    è¾“å…¥æ˜¯ndarryï¼Œè¡Œä¿¡å·
    è¾“å‡ºè¿˜è¦ndarryï¼Œè¡Œä¿¡å·
    """
    data_final = np.zeros_like(data)
    for i in range(data.shape[0]):
        data_final[i] = np.array(SPA_detrending(data[i].tolist()))
    return data_final

def SPA_detrending(data, mu=1200):
    """
    å¹³æ»‘å…ˆéªŒæ³•å»é™¤è¶‹åŠ¿ (Smoothness Priors Approach, SPA)
    # èŠ¬å…°åº“å¥¥çš®å¥¥å¤§å­¦çš„Karjalainenåšå£«æå‡ºçš„ä¸€ç§ä¿¡å·éçº¿æ€§å»è¶‹åŠ¿æ–¹æ³•ï¼šhttps://m.hanspub.org/journal/paper/28723
    :param mu: æ­£åˆ™åŒ–ç³»æ•°
    :return:
    è¾“å…¥æ˜¯ä¸€ç»´list
    """
    N = len(data)

    D = np.zeros((N - 2, N))
    for n in range(N - 2):
        D[n, n], D[n, n + 1], D[n, n + 2] = 1.0, -2.0, 1.0
    D = mu * np.dot(D.T, D)
    for n in range(len(D)):
        D[n, n] += 1.0
    L = sl.cholesky(D, lower=True)
    Y = sl.solve_triangular(L, data, trans='N', lower=True)
    y = sl.solve_triangular(L, Y, trans='T', lower=True)
    data -= y
    return data.tolist()



def Filter(data):
    """
    è¾“å…¥æ˜¯ndarryï¼Œè¡Œä¿¡å·
    è¾“å‡ºè¿˜è¦ndarryï¼Œè¡Œä¿¡å·
    """
    data_final = np.zeros_like(data)
    for i in range(data.shape[0]):
        data_final[i] = np.array(bandPassFilter(data[i].tolist()))
    return data_final

def bandPassFilter(data):
    """
    å¸¦é€šæ»¤æ³¢å™¨ 0.667--2.5Hz

    è¿™é‡Œå‡è®¾é‡‡æ ·é¢‘ç‡ä¸º1000hz,ä¿¡å·æœ¬èº«æœ€å¤§çš„é¢‘ç‡ä¸º500hzï¼Œè¦æ»¤é™¤100hzä»¥ä¸‹ï¼Œ
    400hzä»¥ä¸Šé¢‘ç‡æˆåˆ†ï¼Œå³æˆªè‡³é¢‘ç‡ä¸º100ï¼Œ400hz,åˆ™wn1=2*100/1000=0.2ï¼ŒWn1=0.2
    wn2=2*400/1000=0.8ï¼ŒWn2=0.8ã€‚Wn=[0.2,0.8]
    :return:
    """
    wn1 = 2 * 0.8 / 30   # origin 0.667
    wn2 = 2 * 2.0 / 30
    b, a = signal.butter(N=8, Wn=[wn1, wn2], btype='bandpass')     # 8é˜¶
    data = signal.filtfilt(b, a, data)                        # dataä¸ºè¦è¿‡æ»¤çš„ä¿¡å·
    data = data.tolist()                                 # ndarray --> list
    return data



# IOUè®¡ç®—å‡½æ•°
def cal_iou(box1, box2):
    """
    :param box1: = [xmin1, ymin1, xmax1, ymax1]
    :param box2: = [xmin2, ymin2, xmax2, ymax2]
    :return:
    """
    [[xmin1, ymin1], [xmax1, ymin1], [xmin1, ymax1], [xmax1, ymax1]] = box1[0, :, :]
    [[xmin2, ymin2], [xmax2, ymin2], [xmin2, ymax2], [xmax2, ymax2]] = box2[0, :, :]
    # xmin2, ymin2, xmax2, ymax2 = box2[0,:,:]
    # è®¡ç®—æ¯ä¸ªçŸ©å½¢çš„é¢ç§¯
    s1 = (xmax1 - xmin1) * (ymax1 - ymin1)  # b1çš„é¢ç§¯
    s2 = (xmax2 - xmin2) * (ymax2 - ymin2)  # b2çš„é¢ç§¯

    # è®¡ç®—ç›¸äº¤çŸ©å½¢
    xmin = max(xmin1, xmin2)
    ymin = max(ymin1, ymin2)
    xmax = min(xmax1, xmax2)
    ymax = min(ymax1, ymax2)

    w = max(0, xmax - xmin)
    h = max(0, ymax - ymin)
    a1 = w * h  # Câˆ©Gçš„é¢ç§¯
    a2 = s1 + s2 - a1
    iou = a1 / a2  # iou = a1/ (s1 + s2 - a1)
    return iou



def Wavelet2(ppg):
    """
    å°æ³¢å»å™ª https://www.jianshu.com/p/74f8e6d35ad5
    """
    y = ppg
    x= range(len(y))
    coeffs = pywt.wavedec(y, 'db4', level=4)  # 4é˜¶å°æ³¢åˆ†è§£
    ya4 = pywt.waverec(np.multiply(coeffs, [1, 0, 0, 0, 0]).tolist(), 'db4')
    yd4 = pywt.waverec(np.multiply(coeffs, [0, 1, 0, 0, 0]).tolist(), 'db4')
    yd3 = pywt.waverec(np.multiply(coeffs, [0, 0, 1, 0, 0]).tolist(), 'db4')
    yd2 = pywt.waverec(np.multiply(coeffs, [0, 0, 0, 1, 0]).tolist(), 'db4')
    yd1 = pywt.waverec(np.multiply(coeffs, [0, 0, 0, 0, 1]).tolist(), 'db4')




def Wavelet(ppg):
    """
    å°æ³¢å»å™ª https://www.cnblogs.com/sggggr/p/12381164.html
    è¾“å…¥ï¼š
    è¾“å‡ºï¼š
    """
    index = []
    data = []
    for i in range(len(ppg) - 1):
        X = float(i)
        Y = float(ppg[i])
        index.append(X)
        data.append(Y)

    # Create wavelet object and define parameters
    w = pywt.Wavelet('db8')  # é€‰ç”¨Daubechies8å°æ³¢
    maxlev = pywt.dwt_max_level(len(data), w.dec_len)
    print("maximum level is " + str(maxlev))
    threshold = 0.01  # Threshold for filtering

    # Decompose into wavelet components, to the level selected:
    coeffs = pywt.wavedec(data, 'db8', level=maxlev)  # å°†ä¿¡å·è¿›è¡Œå°æ³¢åˆ†è§£

    for i in range(1, len(coeffs)):
        coeffs[i] = pywt.threshold(coeffs[i], threshold * max(coeffs[i]))  # å°†å™ªå£°æ»¤æ³¢

    mintime = 0
    maxtime = mintime + len(data)

    datarec = pywt.waverec(coeffs, 'db8')  # å°†ä¿¡å·è¿›è¡Œå°æ³¢é‡æ„
    final_data = np.array(data[mintime:maxtime]) - np.array(datarec[mintime:maxtime])

    return final_data.tolist()



class KalmanFilter(object):
    """
    https://github.com/zziz/kalman-filter
    """
    def __init__(self, F = None, B = None, H = None, Q = None, R = None, P = None, x0 = None):

        if(F is None or H is None):
            raise ValueError("Set proper system dynamics.")

        self.n = F.shape[1]
        self.m = H.shape[1]

        self.F = F
        self.H = H
        self.B = 0 if B is None else B
        self.Q = np.eye(self.n) if Q is None else Q
        self.R = np.eye(self.n) if R is None else R
        self.P = np.eye(self.n) if P is None else P
        self.x = np.zeros((self.n, 1)) if x0 is None else x0

    def predict(self, u = 0):
        self.x = np.dot(self.F, self.x) + np.dot(self.B, u)
        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q
        return self.x

    def update(self, z):
        y = z - np.dot(self.H, self.x)
        S = self.R + np.dot(self.H, np.dot(self.P, self.H.T))
        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))
        self.x = self.x + np.dot(K, y)
        I = np.eye(self.n)
        self.P = np.dot(np.dot(I - np.dot(K, self.H), self.P),
        	(I - np.dot(K, self.H)).T) + np.dot(np.dot(K, self.R), K.T)



def ICA_compute(data):
    """
    è¾“å…¥ï¼šåº”è¯¥æ˜¯array
    è¾“å‡ºï¼š
    """
    ica = FastICA(n_components=5)  # å¾—åˆ°å‡ ä¸ªè¾“å‡º?
    data = data.T  # éœ€è¦è¾“å…¥åˆ—ä¿¡å·
    u = ica.fit_transform(data)  # icaåä¹Ÿæ˜¯åˆ—å‘é‡ä¿¡å·
    u = u.T
    return u  # è¿”å›è¡Œä¿¡å·array



# ç§»åŠ¨å¹³å‡æ»¤æ³¢, æ»¤æ³¢å™¨é•¿åº¦=3ï¼Œå¹¶å»ç›´æµï¼Œç›¸å½“äºæŠ‘åˆ¶é«˜é¢‘ï¼Ÿ
def moveAverageFilter(data):
    """
    è¾“å…¥æ˜¯list?
    è¾“å‡ºæ˜¯
    """
    data1 = []
    j = 0
    while j < len(data):
        a = data[j] + data[j + 1] + data[j + 2] if j + 1 < (len(data) - 1) else data[j] * 3  # è¯­æ³•?ï¼Œæœ€åä¸¤ä¸ªå…ƒç´ ä¸å˜äº†
        data1.append(a / 3)
        j += 1
    data = data1
    # å»ç›´æµ
    direct_current = np.mean(data)  # å¯¹ç§»åŠ¨å¹³å‡æ»¤æ³¢åçš„æ•°æ®å–å¹³å‡
    data1 = [j - direct_current for j in data]  # ?
    data = data1
    return data



# æ»‘åŠ¨çª—å£æ»¤æ³¢ç®—æ³•ï¼Ÿå½’ä¸€åŒ–ï¼Œåœ¨0åæ ‡è½´é™„è¿‘éœ‡è¡
def sliding_window_demean(signal_values, num_windows):
    """
    è¾“å…¥ç±»å‹
    """
    window_size = int(round(len(signal_values) / num_windows))
    demeaned = np.zeros(signal_values.shape)
    for i in range(0, len(signal_values), window_size):
        if i + window_size > len(signal_values):
            window_size = len(signal_values) - i
        curr_slice = signal_values[i: i + window_size]
        demeaned[i:i + window_size] = curr_slice - np.mean(curr_slice)
    return demeaned



# å…¨å±€è‡ªç›¸å…³æ»¤æ³¢ï¼Œç”¨è‡ªç›¸å…³å¤„ç†åŒ…å«éšæœºå™ªå£°çš„ä¿¡å·ã€‚åº”è¯¥æ˜¯å‡ºè‡ªï¼š
# RealSense = Real Heart Rate: Illumination Invariant Heart Rate Estimation from Videos
def globalSelfsimilarityFilter(data):
    acf1 = np.correlate(data, data, mode='full')  # np.correlateç”¨äºè®¡ç®—ä¸¤ä¸ªä¸€ç»´åºåˆ—çš„äº’ç›¸å…³
    acf1 = acf1[len(data) - 1:]
    acf1 = acf1 / np.arange(len(data), 0, -1)
    acf1 = acf1 / acf1[0]
    data = acf1
    return data.tolist()


# 2 å¹³æ»‘å…ˆéªŒæ³• (Smoothness Priors Approach, SPA)|param mu: æ­£åˆ™åŒ–ç³»æ•°|å»é™¤è¶‹åŠ¿ï¼Œç›¸å½“äºå»é™¤ä½é¢‘
# èŠ¬å…°åº“å¥¥çš®å¥¥å¤§å­¦çš„Karjalainenåšå£«æå‡ºçš„ä¸€ç§ä¿¡å·éçº¿æ€§å»è¶‹åŠ¿æ–¹æ³•ï¼šhttps://m.hanspub.org/journal/paper/28723
def SPA_detrending1(data, mu=1200):
    """
    å¹³æ»‘å…ˆéªŒæ³•å»é™¤è¶‹åŠ¿ (Smoothness Priors Approach, SPA),åº†éºŸåæ¥çš„
    :param mu: æ­£åˆ™åŒ–ç³»æ•°
    :return:
    """
    N = len(data)
    D = np.zeros((N - 2, N))
    for n in range(N - 2):
        D[n, n], D[n, n + 1], D[n, n + 2] = 1.0, -2.0, 1.0
    D = mu * np.dot(D.T, D)
    for n in range(len(D)):
        D[n, n] += 1.0
    L = sl.cholesky(D, lower=True)
    Y = sl.solve_triangular(L, data, trans='N', lower=True)
    y = sl.solve_triangular(L, Y, trans='T', lower=True)
    data -= y
    return data


# arctanå½’ä¸€åŒ–
def arctan_Normalization(data):
    """
    è¾“å…¥ï¼š
    è¾“å‡ºï¼š
    """
    data = [x*4 for x in data]
    data = np.arctan(data)
    data = data.tolist()
    return data


def amplitudeSelectiveFiltering(C_rgb, amax=0.002, delta=0.0001):
    """
    å¹…å€¼æ»¤æ³¢
    https://github.com/PartyTrix/Amplitude_selective_filtering
    Input: Raw RGB signals with dimensions 3xL, where the R channel is column 0
    Output:
    C = Filtered RGB-signals with added global mean,
    raw = Filtered RGB signals
    """
    C_rgb  = np.array(C_rgb)
    C_rgb = np.expand_dims(C_rgb, axis=1)

    s = C_rgb.shape[1]  # åˆ—æ•°ï¼Œå³æ—¶é—´åºåˆ—é•¿åº¦
    c = (1 / (np.mean(C_rgb, 1)))  # å¯¹å„è¡Œæ±‚å‡å€¼ï¼Œå†æ±‚å€’æ•°ï¼Œ æ˜¯ä¸€ç»´æ•°ç»„äº†ï¼Œé•¿åº¦ä¸º3

    # line 1
    c = np.transpose(np.array([c, ] * s)) * C_rgb - 1  # ç±»ä¼¼å¯¹æ¯ä¸ªæ—¶åºä¿¡å·è¿›è¡Œå½’ä¸€åŒ–

    # line 2
    f = abs(np.fft.fft(c, n=s, axis=1) / s)  # L -> C_rgb.shape[0],å¯¹æ¯ä¸€è¡Œåˆ†åˆ«è¿›è¡ŒFFTå˜æ¢ï¼Œå†é™¤ä»¥æ—¶åºä¿¡å·é•¿åº¦ï¼Œæœ€åå–ç»å¯¹å€¼

    # line 3
    w = (delta / np.abs(f[0, :]))  # F[0,:]  is the R-channel ï¼Œçº¢è‰²é€šé“å–å€’æ•°å†ä¹˜ä»¥ğŸ”ºï¼Œshapeå’Œtypeæ˜¯ï¼Ÿ

    # line 4
    w[np.abs(f[0, :] < amax)] = 1  # å°äºæœ€å¤§å€¼çš„å–1
    w = w.reshape([1, s])

    # line 5
    # ff = np.multiply(f, (np.tile(w, [3, 1])))  # å¤åˆ¶ä¸‰è¡Œï¼Œå†ä¸fé€å…ƒç´ ç›¸ä¹˜
    ff = np.multiply(f, (np.tile(w, [1, 1])))  # å¤åˆ¶ä¸‰è¡Œï¼Œå†ä¸fé€å…ƒç´ ç›¸ä¹˜

    # line 6
    c = np.transpose(np.array([(np.mean(C_rgb, 1)), ] * s)) * np.abs(np.fft.ifft(ff) + 1)  # å‡å€¼arrayä¸ FFTé€†å˜æ¢åŠ 1çš„å€¼ é€å…ƒç´ ç›¸ä¹˜
    raw = np.abs(np.fft.ifft(ff) + 1)

    return c.T, raw.T  # è½¬ç½®åå†è¾“å‡º



# 5å½’ä¸€åŒ–
def arctanNormalization(self):
    self.data = [x*4 for x in self.data]
    self.data = np.arctan(self.data)
    self.data = self.data.tolist()

    plt.figure(2)  # ç”»å›¾
    plt.plot(self.data)
    plt.title('arctan normalization')


# 3ä¸»æˆåˆ†åˆ†æ,å¾—åˆ°ä¸€ç»´ä¸»ä½ç½®ä¿¡å·(é™ç»´)
def PCA_compute(data):
    """
    Perform PCA on the time series data and project the points onto the
    principal axes of variation (eigenvectors of covariance matrix) to get
    the principal 1-D position signals
    è¾“å…¥æ˜¯ndarray,è¡Œä¿¡å·
    """
    # Object for principal component analysisä¸»æˆåˆ†åˆ†æå¯¹è±¡
    pca = PCA(n_components=5)
    temp = data.T  # Arrange the time series data in the format given in the paper
    l2_norms = np.linalg.norm(temp, ord=2, axis=1)  # Get L2 norms of each m_t
    # æŠ›å¼ƒåœ¨å‰25åä¸­æœ‰L2æ ‡å‡†çš„åˆ†æ•°
    temp_with_abnormalities_removed = temp[l2_norms < np.percentile(l2_norms, 75)]
    # æ‹ŸåˆPCAæ¨¡å‹
    pca.fit(temp_with_abnormalities_removed)
    # å°†è·Ÿè¸ªçš„ç‚¹è¿åŠ¨æŠ•å½±åˆ°ä¸»åˆ†é‡å‘é‡ä¸Š
    projected = pca.transform(temp)
    return projected


# 10å¥‡å¼‚å€¼åˆ†è§£SVD
def optimal_svd(Y):
    ### OPTIMAL SHRINKAGE SVD
    # Implementation of algorithm proposed in:
    # based on the following paper: Gavish, Matan, and David L. Donoho. "Optimal shrinkage of singular values." IEEE Transactions on Information Theory 63.4 (2017): 2137-2152.
    #
    U, s, V = np.linalg.svd(Y, full_matrices=False)
    m, n = Y.shape
    beta = m / n

    y_med = np.median(s)

    beta_m = (1 - np.sqrt(beta)) ** 2
    beta_p = (1 + np.sqrt(beta)) ** 2

    t_array = np.linspace(beta_m, beta_p, 100000)
    dt = np.diff(t_array)[0]

    f = lambda t: np.sqrt((beta_p - t) * (t - beta_m)) / (2 * np.pi * t * beta)
    F = lambda t: np.cumsum(f(t) * dt)

    mu_beta = t_array[np.argmin((F(t_array) - 0.5) ** 2)]

    sigma_hat = y_med / np.sqrt(n * mu_beta)

    def eta(y, beta):
        mask = (y >= (1 + np.sqrt(beta)))
        aux_sqrt = np.sqrt((y[mask > 0] ** 2 - beta - 1) ** 2 - 4 * beta)
        aux = np.zeros(y.shape)
        aux[mask > 0] = aux_sqrt / y[mask > 0]
        return mask * aux

    def eta_sigma(y, beta, sigma):
        return sigma * eta(y / sigma, beta)

    s_eta = eta_sigma(s, beta, sigma_hat * np.sqrt(n))

    #     trim U,V
    aux = s_eta > 0

    return U[:, aux], s_eta[aux], V[aux, :]


# é€‰æ‹©æœ€å…·æœ‰å‘¨æœŸæ€§çš„ä¿¡å·ï¼Œå³åŠŸç‡è°±å¯†åº¦å³°å€¼ç‚¹æ¨ªåæ ‡å¯¹åº”çš„é¢‘ç‡
def signal_selection(s):
    """
    å¿ƒç‡æ˜¯é€šè¿‡é€‰æ‹©æœ€å…·æœ‰å‘¨æœŸä¿¡å·çš„æœ€å¤§é¢‘ç‡æ¥è®¡ç®—çš„
    sï¼šåˆ—ä¿¡å·
    """
    fs = 30
    percentages = np.zeros(5)  # ä¸€ç»´åº¦ï¼Œ5ä¸ª0
    # æœ¬æ–‡åªåˆ†æäº†å‰5ä¸ªä¿¡å·
    for i in range(5):
        s_i = s[:, i]  # æœç„¶è¿˜æ˜¯åˆ—
        # è®¡ç®—æºä¿¡å·çš„åŠŸç‡è°±ï¼Œreturnçš„æ˜¯ä¸¤ä¸ªarrayï¼Œä¸çŸ¥é“ç»´åº¦ flattopæ˜¯çª—å£ç±»å‹ï¼Œspectrumæ˜¯åŠŸç‡è°±å¯†åº¦
        f, Pxx_spec = signal.periodogram(s_i, fs, 'flattop', scaling='spectrum')  # fçš„shapeä¸€ç»´lenï¼š251,èŒƒå›´0åˆ°15. Pxx_specä¹Ÿæ˜¯ï¼Œä½†å¥½åƒç«–ç€æ’åˆ—
        total_power = np.sum(Pxx_spec)  # è®¡ç®—æ€»åŠŸç‡
        # å¾—åˆ°æœ€å¤§åŠŸç‡çš„é¢‘ç‡åŠå…¶ä¸€æ¬¡è°æ³¢ï¼Œè¿˜æœ‰å®ƒä»¬çš„åæ ‡ç´¢å¼•
        [minPower, maxPower, minLoc, maxLoc] = cv2.minMaxLoc(Pxx_spec)  # ï¼Ÿ
        freqMaxPower = f[maxLoc[1]]  # maxLocæ˜¯tupleç±»å‹(0, 17)
        firstHarmonic = 2 * freqMaxPower
        firstHarmonicLoc = np.where(f == firstHarmonic)
        firstHarmonicPower = Pxx_spec[firstHarmonicLoc]
        #  è®¡ç®—æœ€å¤§é¢‘ç‡å æ€»åŠŸç‡çš„ç™¾åˆ†æ¯” å–å‰äº”ä¸ª
        percentages[i] = (maxPower + firstHarmonicPower) / total_power
    # åœ¨äº”ä¸ªåˆ†é‡ä¸­ï¼Œä»æœ€å…·å‘¨æœŸçš„ä¿¡å·æ¥è®¡ç®—BPM
    most_periodic_signal = np.argmax(percentages)  # è¿”å›æœ€å¤§å€¼ç´¢å¼•
    selected_signal = s[:, most_periodic_signal]  # æŒ‘å‡ºæ¥çš„æœ€å…·æœ‰å‘¨æœŸæ€§çš„ä¿¡å·
    f, Pxx_spec = signal.periodogram(selected_signal, fs, 'flattop', scaling='spectrum')
    [minVal, maxVal, minLoc, maxLoc] = cv2.minMaxLoc(Pxx_spec)  # è·å¾—æœ€å¤§åŠŸç‡æœ€å¤§å€¼çš„ç´¢å¼•ï¼Ÿ
    f_pulse = f[maxLoc[1]]
    bpm = 60.0 * f_pulse
    return bpm, selected_signal


# 3FFTå˜æ¢,è®¡ç®—è„‰æç‡å¹¶å±•ç¤ºç»“æœ
def fftTransfer1(filtered, framerate=30):  # è¾“å…¥æ•°æ®å’Œå¸§ç‡:ä¿¡å·æŠ½æ ·ç‡
    n = 512
    if len(filtered) < n:
        for _ in range(n - len(filtered)):  # è¡¥é›¶è¡¥è‡³Nç‚¹
            filtered.append(0)
    else:
        filtered = filtered[0:n]
    df = [framerate / n * i for i in range(n)]  # é¢‘è°±åˆ†è¾¨ç‡ï¼Ÿ
    fft_data = np.abs(np.fft.fft(filtered))

    hr = df[fft_data.tolist()[0:100].index(max(fft_data.tolist()[0:100]))] * 60  # å³°å€¼æ¨ªåæ ‡
    print('HR estimate:', hr)
    return hr



# æ‰¾åˆ°é¢‘åŸŸæœ€å¤§å€¼å¯¹åº”çš„é¢‘ç‡ï¼Œæ‰¾æœ€å¤§ä¸¤ç§æ–¹æ³•ç»“æœç«Ÿç„¶ä¸ä¸€æ ·
# è¿™ä¸ªæ–¹æ³•è¾ƒå·®,å¯èƒ½æ˜¯fç²¾åº¦ä¸å¤Ÿ
def get_HR(f_signal, samplerate):  # f, pxx_specéƒ½æ˜¯251é•¿åº¦ä¸€ç»´ndarray
    f, pxx_spec = signal.periodogram(f_signal, samplerate, 'flattop', scaling='spectrum')
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(pxx_spec)
    hr = f[max_loc[1]]
    bpm = 60.0 * hr
    return bpm


# 12å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰è®¡ç®—
def compute_RMSE_every_n(signal1, signal2, sampling_time, mean_time):
    signal1_m, time_m = strided_mean(signal1, sampling_time, mean_time)
    signal2_m, _ = strided_mean(signal2, sampling_time, mean_time)

    RMSE = np.sqrt(np.mean((signal1_m - signal2_m) ** 2))
    rRMSE = np.mean(np.abs(signal1_m - signal2_m) / signal1_m)
    return RMSE, rRMSE


# å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰è®¡ç®—è°ƒç”¨
def strided_mean(signal,sampling_time, mean_time):
   block_len = np.ceil(mean_time/sampling_time)
   n_blocks = np.floor(signal.shape[0]/block_len)
   mean_sig = np.zeros([int(n_blocks),1])
   mean_t =np.arange(n_blocks)*mean_time
   for i in np.arange(n_blocks):
       mean_sig[int(i)]= signal[int(i*block_len):int((i+1)*block_len),...].mean()
   return mean_sig, mean_t

